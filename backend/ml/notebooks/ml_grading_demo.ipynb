{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0db6cf",
   "metadata": {},
   "source": [
    "# Auto-Grader ML System Demonstration\n",
    "\n",
    "This notebook demonstrates the machine learning capabilities of the Auto-Grader system. It includes model training, evaluation, and prediction examples to show how the system grades student submissions automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f31392",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70da77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our ML modules\n",
    "from utils.train_models import DataProcessor, ModelTrainer, ModelManager\n",
    "from utils.grade_engine import GradingEngine\n",
    "from utils.feedback_generator import FeedbackGenerator, FeedbackFormatter\n",
    "from utils.model_manager import ModelVersion, ABTestManager\n",
    "\n",
    "# Set up directories\n",
    "MODEL_DIR = '../models'\n",
    "ASSIGNMENTS_DIR = '../../storage/nbgrader_assignments'\n",
    "FEEDBACK_DIR = '../../storage/nbgrader_feedback'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(ASSIGNMENTS_DIR, exist_ok=True)\n",
    "os.makedirs(FEEDBACK_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e71623c",
   "metadata": {},
   "source": [
    "## 2. Generate Sample Training Data\n",
    "\n",
    "For demonstration purposes, we'll create some sample training data to simulate NBGrader assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa308fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to generate sample data\n",
    "def create_sample_training_data():\n",
    "    \"\"\"\n",
    "    Create sample training data for demonstration\n",
    "    \"\"\"\n",
    "    # Define questions and answers\n",
    "    questions = [\n",
    "        {\n",
    "            \"id\": \"q1\",\n",
    "            \"text\": \"What is the difference between supervised and unsupervised learning?\",\n",
    "            \"sample_answer\": \"Supervised learning uses labeled data where the algorithm learns to map inputs to known outputs. It requires a training dataset with correct answers (labels). Examples include classification and regression problems. Unsupervised learning, on the other hand, works with unlabeled data and tries to find patterns or structure without predefined outputs. Common unsupervised learning techniques include clustering, dimensionality reduction, and association.\",\n",
    "            \"keywords\": [\"labeled\", \"unlabeled\", \"classification\", \"clustering\", \"regression\", \"training\"],\n",
    "            \"max_score\": 10.0,\n",
    "            \"grading_rubric\": {\n",
    "                \"10\": \"Complete explanation with clear distinction between both types and examples\",\n",
    "                \"8\": \"Good explanation with basic distinction and some examples\",\n",
    "                \"6\": \"Basic distinction between supervised and unsupervised learning\",\n",
    "                \"4\": \"Partial explanation with some confusion\",\n",
    "                \"2\": \"Very limited explanation with major misconceptions\",\n",
    "                \"0\": \"No answer or completely incorrect\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q2\",\n",
    "            \"text\": \"Explain how the backpropagation algorithm works in neural networks.\",\n",
    "            \"sample_answer\": \"Backpropagation is an algorithm used to train neural networks by calculating gradients. It works in two phases: forward pass and backward pass. In the forward pass, input data is fed through the network to generate predictions. The prediction error is calculated using a loss function. In the backward pass, the algorithm calculates the gradient of the loss function with respect to each weight by applying the chain rule of calculus, propagating the error backward through the network. The weights are then updated using gradient descent to minimize the loss. This process is repeated iteratively until the model converges to an optimal solution.\",\n",
    "            \"keywords\": [\"gradient\", \"forward pass\", \"backward pass\", \"chain rule\", \"weights\", \"loss function\"],\n",
    "            \"max_score\": 10.0,\n",
    "            \"grading_rubric\": {\n",
    "                \"10\": \"Comprehensive explanation including forward pass, backward pass, chain rule, and gradient descent\",\n",
    "                \"8\": \"Good explanation with most key concepts mentioned\",\n",
    "                \"6\": \"Basic explanation of backpropagation process\",\n",
    "                \"4\": \"Partial explanation with some key concepts missing\",\n",
    "                \"2\": \"Very limited explanation with major misconceptions\",\n",
    "                \"0\": \"No answer or completely incorrect\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"q3\",\n",
    "            \"text\": \"What is the bias-variance tradeoff in machine learning?\",\n",
    "            \"sample_answer\": \"The bias-variance tradeoff is a fundamental concept in machine learning that describes the balancing act between creating models that are too simple (high bias) versus too complex (high variance). High bias models underfit the data, making strong assumptions and simplifying the target function, resulting in high error on both training and test data. High variance models overfit the data, capturing noise along with the underlying pattern, performing well on training data but poorly on test data. The goal is to find the sweet spot that minimizes total error. Techniques to manage this tradeoff include regularization, cross-validation, and ensemble methods. The ideal model complexity depends on the amount and quality of training data available.\",\n",
    "            \"keywords\": [\"underfitting\", \"overfitting\", \"complexity\", \"regularization\", \"error\", \"model selection\"],\n",
    "            \"max_score\": 10.0,\n",
    "            \"grading_rubric\": {\n",
    "                \"10\": \"Complete explanation including definition, consequences of high bias/variance, and methods to balance the tradeoff\",\n",
    "                \"8\": \"Good explanation with clear understanding of overfitting and underfitting\",\n",
    "                \"6\": \"Basic explanation of bias-variance tradeoff concept\",\n",
    "                \"4\": \"Partial explanation with some misconceptions\",\n",
    "                \"2\": \"Very limited explanation\",\n",
    "                \"0\": \"No answer or completely incorrect\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(questions)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate and display sample data\n",
    "training_data = create_sample_training_data()\n",
    "print(f\"Generated {len(training_data)} sample questions\")\n",
    "training_data[['id', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6971f",
   "metadata": {},
   "source": [
    "## 3. Train ML Models\n",
    "\n",
    "Now we'll train our machine learning models using the sample data. We'll train both a similarity-based model and a transformer-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8df7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "data_processor = DataProcessor()\n",
    "model_trainer = ModelTrainer(MODEL_DIR)\n",
    "\n",
    "# Generate training pairs from the sample data\n",
    "X, y = data_processor.generate_training_pairs(training_data)\n",
    "print(f\"Generated {len(X)} training pairs from sample data\")\n",
    "\n",
    "# Display a few examples\n",
    "for i in range(min(3, len(X))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {X[i][0][:100]}...\")\n",
    "    print(f\"Answer: {X[i][1][:100]}...\")\n",
    "    print(f\"Score: {y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7726ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train similarity model\n",
    "print(\"Training similarity model...\")\n",
    "similarity_model = model_trainer.train_similarity_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Save model\n",
    "model_path = model_trainer.save_model(similarity_model, 'similarity')\n",
    "print(f\"Saved similarity model to {model_path}\")\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nModel performance:\")\n",
    "for metric, value in similarity_model['metrics'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835b0e9",
   "metadata": {},
   "source": [
    "## 4. Grade Student Submissions\n",
    "\n",
    "Let's use our trained models to grade some sample student submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize grading engine\n",
    "grading_engine = GradingEngine(MODEL_DIR)\n",
    "\n",
    "# Sample student submissions\n",
    "student_submissions = [\n",
    "    {\n",
    "        \"id\": \"submission1\",\n",
    "        \"question_id\": \"q1\",\n",
    "        \"answer\": \"Supervised learning uses labeled data with inputs and outputs. Unsupervised learning uses unlabeled data to find patterns. Examples of supervised learning are classification and regression. Examples of unsupervised learning include clustering.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"submission2\",\n",
    "        \"question_id\": \"q1\",\n",
    "        \"answer\": \"Supervised learning is when the computer is given examples to learn from. Unsupervised learning is when it learns on its own.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"submission3\",\n",
    "        \"question_id\": \"q2\",\n",
    "        \"answer\": \"Backpropagation works by calculating gradients of the loss function with respect to weights. It uses the chain rule to propagate errors backward through the network, updating weights to minimize the loss function. The process includes a forward pass where predictions are made and a backward pass where errors are propagated back.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare questions dictionary\n",
    "questions = {q['id']: q for q in training_data.to_dict('records')}\n",
    "\n",
    "# Grade submissions\n",
    "grading_results = grading_engine.batch_grade_submissions(student_submissions, questions)\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(grading_results):\n",
    "    print(f\"\\nSubmission {i+1} (ID: {result['submission_id']})\")\n",
    "    print(f\"Question: {questions[result['question_id']]['text'][:100]}...\")\n",
    "    print(f\"Score: {result['score']} / {result['max_score']} (Confidence: {result['confidence']:.2f})\")\n",
    "    print(\"Feedback:\")\n",
    "    for feedback in result['feedback']:\n",
    "        print(f\"- {feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c043b",
   "metadata": {},
   "source": [
    "## 5. Generate Detailed Feedback\n",
    "\n",
    "Now let's generate more detailed feedback for one of the submissions using our feedback generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ac051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feedback generator\n",
    "feedback_generator = FeedbackGenerator()\n",
    "\n",
    "# Select a submission\n",
    "submission_idx = 0  # First submission\n",
    "submission = student_submissions[submission_idx]\n",
    "result = grading_results[submission_idx]\n",
    "question = questions[submission['question_id']]\n",
    "\n",
    "# Generate feedback\n",
    "missing_concepts = [\"The need for labeled training data in supervised learning\", \n",
    "                   \"How supervised learning maps inputs to known outputs\"]\n",
    "missing_keywords = [\"training\", \"regression\"]\n",
    "\n",
    "feedback = feedback_generator.generate_feedback(\n",
    "    student_answer=submission['answer'],\n",
    "    expected_answer=question['sample_answer'],\n",
    "    score=result['score'],\n",
    "    max_score=result['max_score'],\n",
    "    missing_concepts=missing_concepts,\n",
    "    missing_keywords=missing_keywords\n",
    ")\n",
    "\n",
    "# Format feedback as HTML\n",
    "html_feedback = FeedbackFormatter.format_as_html(feedback, result['score'], result['max_score'])\n",
    "\n",
    "# Display feedback\n",
    "from IPython.display import HTML\n",
    "display(HTML(html_feedback))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fcfa97",
   "metadata": {},
   "source": [
    "## 6. Model Versioning and A/B Testing\n",
    "\n",
    "Let's demonstrate the model versioning and A/B testing capabilities of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c18942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "from utils.model_manager import ModelManager\n",
    "model_manager = ModelManager(MODEL_DIR)\n",
    "\n",
    "# List model versions\n",
    "versions = model_manager.list_versions()\n",
    "print(f\"Found {len(versions)} model versions:\")\n",
    "for v in versions:\n",
    "    print(f\"- {v['model_type']} {v['version']} {'(active)' if v.get('is_active') else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4450ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have at least one similarity model, let's train another version for comparison\n",
    "similarity_versions = [v for v in versions if v['model_type'] == 'similarity']\n",
    "if similarity_versions:\n",
    "    print(\"Training a new version of the similarity model...\")\n",
    "    \n",
    "    # Train with different parameters\n",
    "    # For demonstration, we'll use the same data but pretend it's a new version\n",
    "    similarity_model_v2 = model_trainer.train_similarity_model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # Manually adjust metrics to show difference\n",
    "    similarity_model_v2['metrics']['accuracy'] = similarity_model_v2['metrics']['accuracy'] * 1.05\n",
    "    similarity_model_v2['metrics']['f1_score'] = similarity_model_v2['metrics']['f1_score'] * 1.08\n",
    "    \n",
    "    # Save as new version\n",
    "    model_path = model_trainer.save_model(similarity_model_v2, 'similarity')\n",
    "    print(f\"Saved new similarity model to {model_path}\")\n",
    "    \n",
    "    # Refresh model list\n",
    "    model_manager.load_model_versions()\n",
    "    versions = model_manager.list_versions('similarity')\n",
    "    print(f\"\\nUpdated similarity model versions:\")\n",
    "    for v in versions:\n",
    "        print(f\"- {v['model_type']} {v['version']} {'(active)' if v.get('is_active') else ''}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c948bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up A/B testing\n",
    "similarity_versions = [v for v in versions if v['model_type'] == 'similarity']\n",
    "if len(similarity_versions) >= 2:\n",
    "    # Initialize A/B test manager\n",
    "    ab_manager = ABTestManager(model_manager)\n",
    "    \n",
    "    # Create A/B test\n",
    "    version_a = similarity_versions[0]['version']\n",
    "    version_b = similarity_versions[1]['version']\n",
    "    \n",
    "    test_config = ab_manager.create_test(\n",
    "        test_name=\"Similarity Model Comparison\",\n",
    "        model_type=\"similarity\",\n",
    "        version_a=version_a,\n",
    "        version_b=version_b,\n",
    "        traffic_split=0.5,\n",
    "        metrics=[\"accuracy\", \"f1_score\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"Created A/B test: {test_config['name']} (ID: {test_config['id']})\")\n",
    "    \n",
    "    # Simulate collecting metrics\n",
    "    test_id = test_config['id']\n",
    "    \n",
    "    # Update metrics for version A\n",
    "    version_a_metrics = {\n",
    "        \"accuracy\": 0.82,\n",
    "        \"f1_score\": 0.79\n",
    "    }\n",
    "    ab_manager.update_test_metrics(test_id, version_a, version_a_metrics, sample_count=10)\n",
    "    \n",
    "    # Update metrics for version B\n",
    "    version_b_metrics = {\n",
    "        \"accuracy\": 0.87,\n",
    "        \"f1_score\": 0.85\n",
    "    }\n",
    "    ab_manager.update_test_metrics(test_id, version_b, version_b_metrics, sample_count=10)\n",
    "    \n",
    "    # Get results\n",
    "    results = ab_manager.get_test_results(test_id)\n",
    "    print(\"\\nA/B Test Results:\")\n",
    "    print(json.dumps(results, indent=2))\n",
    "    \n",
    "    # End test\n",
    "    final_results = ab_manager.end_test(test_id)\n",
    "    \n",
    "    # Promote winner\n",
    "    if 'final_metrics' in final_results and 'winner' in final_results['final_metrics']:\n",
    "        winner = final_results['final_metrics']['winner']\n",
    "        if winner:\n",
    "            version = final_results['version_a'] if winner == 'version_a' else final_results['version_b']\n",
    "            print(f\"\\nPromoting {version} as the active version\")\n",
    "            ab_manager.promote_winner(test_id)\n",
    "            \n",
    "            # Refresh model list\n",
    "            model_manager.load_model_versions()\n",
    "            versions = model_manager.list_versions('similarity')\n",
    "            print(f\"\\nUpdated similarity model versions:\")\n",
    "            for v in versions:\n",
    "                print(f\"- {v['model_type']} {v['version']} {'(active)' if v.get('is_active') else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d64cd",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Performance\n",
    "\n",
    "Let's visualize the performance of our models over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f72ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model performance history\n",
    "similarity_history = model_manager.get_model_performance_history('similarity')\n",
    "\n",
    "if similarity_history:\n",
    "    # Convert to DataFrame\n",
    "    history_df = pd.DataFrame(similarity_history)\n",
    "    \n",
    "    # Add created_at as datetime\n",
    "    history_df['created_at'] = pd.to_datetime(history_df['created_at'])\n",
    "    \n",
    "    # Sort by creation date\n",
    "    history_df = history_df.sort_values('created_at')\n",
    "    \n",
    "    # Plot performance metrics\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_df['created_at'], history_df['accuracy'], marker='o', linestyle='-', color='blue')\n",
    "    plt.title('Accuracy Over Time')\n",
    "    plt.xlabel('Version Creation Date')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_df['created_at'], history_df['f1_score'], marker='o', linestyle='-', color='green')\n",
    "    plt.title('F1 Score Over Time')\n",
    "    plt.xlabel('Version Creation Date')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No performance history available for similarity models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d9c82",
   "metadata": {},
   "source": [
    "## 8. End-to-End Grading Example\n",
    "\n",
    "Let's put everything together and demonstrate the end-to-end grading process for a new submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391df04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new student submission\n",
    "new_submission = {\n",
    "    \"id\": \"new_submission\",\n",
    "    \"question_id\": \"q3\",  # Bias-variance tradeoff question\n",
    "    \"answer\": \"\"\"The bias-variance tradeoff is an important concept in machine learning. Bias is when a model makes strong assumptions about the data, which can lead to underfitting where it doesn't capture the true patterns. Variance is when a model is too sensitive to the training data, leading to overfitting where it captures noise rather than just the underlying pattern.\n",
    "    \n",
    "    High bias models are too simple and perform poorly on both training and test data. High variance models are too complex and perform well on training data but poorly on test data. The goal is to find the right balance that minimizes total error.\n",
    "    \n",
    "    Some ways to manage this tradeoff include regularization techniques, cross-validation, and using ensemble methods. The right model complexity depends on how much training data you have.\"\"\"\n",
    "}\n",
    "\n",
    "# Grade the submission\n",
    "print(\"Grading new submission...\")\n",
    "result = grading_engine.grade_submission(\n",
    "    student_answer=new_submission['answer'],\n",
    "    question_text=questions[new_submission['question_id']]['text'],\n",
    "    expected_answer=questions[new_submission['question_id']]['sample_answer'],\n",
    "    max_score=questions[new_submission['question_id']]['max_score'],\n",
    "    rubric=questions[new_submission['question_id']]['grading_rubric'],\n",
    "    keywords=questions[new_submission['question_id']]['keywords']\n",
    ")\n",
    "\n",
    "print(f\"\\nGrading Result:\\nScore: {result['score']} / {result['max_score']} (Confidence: {result['confidence']:.2f})\")\n",
    "print(f\"Methods used: {', '.join(result['methods_used'])}\")\n",
    "print(\"\\nDetailed metrics:\")\n",
    "for metric, value in result.items():\n",
    "    if isinstance(value, (int, float)) and metric not in ['score', 'max_score', 'confidence']:\n",
    "        print(f\"- {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nFeedback:\")\n",
    "for item in result['feedback']:\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000c33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed feedback with our feedback generator\n",
    "print(\"Generating detailed feedback...\")\n",
    "\n",
    "# Extract missing concepts and keywords from the detailed feedback\n",
    "missing_concepts = []\n",
    "missing_keywords = []\n",
    "\n",
    "if 'semantic_similarity' in result['detailed_feedback']:\n",
    "    sem_feedback = result['detailed_feedback']['semantic_similarity']\n",
    "    if 'missing_concepts' in sem_feedback:\n",
    "        missing_concepts = sem_feedback['missing_concepts']\n",
    "\n",
    "if 'keyword_matching' in result['detailed_feedback']:\n",
    "    kw_feedback = result['detailed_feedback']['keyword_matching']\n",
    "    if 'missing_keywords' in kw_feedback:\n",
    "        missing_keywords = kw_feedback['missing_keywords']\n",
    "\n",
    "# Generate enhanced feedback\n",
    "enhanced_feedback = feedback_generator.generate_feedback(\n",
    "    student_answer=new_submission['answer'],\n",
    "    expected_answer=questions[new_submission['question_id']]['sample_answer'],\n",
    "    score=result['score'],\n",
    "    max_score=result['max_score'],\n",
    "    missing_concepts=missing_concepts,\n",
    "    missing_keywords=missing_keywords\n",
    ")\n",
    "\n",
    "# Format as HTML and display\n",
    "html_feedback = FeedbackFormatter.format_as_html(enhanced_feedback, result['score'], result['max_score'])\n",
    "display(HTML(html_feedback))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca4d93",
   "metadata": {},
   "source": [
    "## 9. Evaluate Human vs. ML Grading\n",
    "\n",
    "Let's compare how our ML grading system performs against human grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some sample data with both ML and human grades\n",
    "comparison_data = [\n",
    "    {\n",
    "        \"submission_id\": \"s1\",\n",
    "        \"question_id\": \"q1\",\n",
    "        \"human_score\": 8.5,\n",
    "        \"ml_score\": result['score'] if new_submission['question_id'] == 'q1' else 7.5,\n",
    "        \"confidence\": result['confidence'] if new_submission['question_id'] == 'q1' else 0.85\n",
    "    },\n",
    "    {\n",
    "        \"submission_id\": \"s2\",\n",
    "        \"question_id\": \"q2\",\n",
    "        \"human_score\": 6.0,\n",
    "        \"ml_score\": result['score'] if new_submission['question_id'] == 'q2' else 5.5,\n",
    "        \"confidence\": result['confidence'] if new_submission['question_id'] == 'q2' else 0.75\n",
    "    },\n",
    "    {\n",
    "        \"submission_id\": \"s3\",\n",
    "        \"question_id\": \"q3\",\n",
    "        \"human_score\": 9.0,\n",
    "        \"ml_score\": result['score'] if new_submission['question_id'] == 'q3' else 8.5,\n",
    "        \"confidence\": result['confidence'] if new_submission['question_id'] == 'q3' else 0.90\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Calculate absolute difference\n",
    "comparison_df['diff'] = abs(comparison_df['human_score'] - comparison_df['ml_score'])\n",
    "\n",
    "# Display comparison\n",
    "print(\"Human vs. ML Grading Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Calculate overall metrics\n",
    "mean_diff = comparison_df['diff'].mean()\n",
    "max_diff = comparison_df['diff'].max()\n",
    "within_1_point = (comparison_df['diff'] <= 1.0).mean() * 100\n",
    "\n",
    "print(f\"\\nAverage difference: {mean_diff:.2f} points\")\n",
    "print(f\"Maximum difference: {max_diff:.2f} points\")\n",
    "print(f\"Percentage within 1 point: {within_1_point:.1f}%\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar chart comparing human and ML scores\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(comparison_df))\n",
    "\n",
    "plt.bar(index, comparison_df['human_score'], bar_width, label='Human Score', color='blue', alpha=0.7)\n",
    "plt.bar(index + bar_width, comparison_df['ml_score'], bar_width, label='ML Score', color='green', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Submission')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Human vs. ML Grading Comparison')\n",
    "plt.xticks(index + bar_width / 2, comparison_df['submission_id'])\n",
    "plt.legend()\n",
    "plt.ylim(0, 10)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb3d2c2",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've demonstrated the core ML capabilities of the Auto-Grader system:\n",
    "\n",
    "1. Training ML models for grading student submissions\n",
    "2. Using multiple grading methods (similarity, keyword matching, transformer-based)\n",
    "3. Generating detailed, actionable feedback for students\n",
    "4. Managing model versions and conducting A/B tests\n",
    "5. Evaluating ML grading against human grading\n",
    "\n",
    "**Next steps for the Auto-Grader ML system:**\n",
    "\n",
    "1. **Expand training data**: Incorporate more real student submissions and human-graded examples\n",
    "2. **Support more question types**: Extend to handle code submissions, mathematical equations, diagrams\n",
    "3. **Improve feedback generation**: Create more personalized and educationally valuable feedback\n",
    "4. **Implement active learning**: Allow the system to learn from teacher corrections\n",
    "5. **Develop bias detection**: Ensure fair grading across different student populations\n",
    "6. **Build explainability tools**: Help teachers understand why the system assigned specific grades\n",
    "\n",
    "These improvements will help make the Auto-Grader system more accurate, fair, and educationally effective."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
